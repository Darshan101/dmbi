**. What is Data Preprocessing ? Explain different methods for Data cleansing phase.**
1. Data have quality if they satisfy the requirements of the intended use. There are many factors comprising data quality,        including accuracy, completeness, consistency, etc.
2. Sometimes, the data you wish to analyze by using data mining techniques are incomplete, inaccurate, noisy or inconsistent.
3. There are many possible reasons for faulty data, such as faulty ata collection instruments, human or computer errors, intentional submission of incorrect data by users, data transmission errors, variations in naming conventions and so on.
4. Such data needs to be preprocessed for finding out anomalies and inaccuracies. If data preprocessing is not performed, the         information obtained after data mining may be unreliable.
5. Data cleaning or data cleansing is the first step in data preprocessing and attempts to fill in missing values, smooth out noise while identifying outliers and correct inconsistencies in the data. Following are the different methods for data cleansing phase:
  a.Filling Missing Values:
    i)Ignore the tuple
    ii)Fill in the missing value manually
    iii)Use a global constant to fill in the missing value.
    iv)Use a central tendnecy measure for the attribute to fill in the missing value.
    v)Use the attribute mean or median for all samples belonging to the same class as the given tuple.
    vi)Use the most probable value to fill in the missing value.
  b.Smoothing Noisy Data:
    i)Binning: Binning methods smooth out a sorted data value by consulting its neighborhood, that is the values around it. The   sorted values are distributed into buckets or bins. Smoothing can be applied on the created bins using the bin means, bin medians or the bin boundaries.
    ii)Regression: Data smoothing can also be done using regression, a technique that conforms data values to a function.
    iii)Outlier Analysis: Outliers may be detected by clustering. Similar values are organized into groups or clusters. Intuitively, values that fall outside of the set of clusters may be considered outliers.
  c)Data Cleaning as a Process: It invovles various methods by which data inconsistency can be eliminated and data can be normalized:
    i)Discrepancy Detection
    ii)Data Scrubbing
    iii)Data Auditing

**. Why data preprocessing is required ? Explain different steps involved in the data preprocessing.*

1.Real world data are incomplete when certain attributes or attributes values are missing.
2.When the data contains errors or some outliers it is considered to be noisy data.
3.When the data contains differences in codes or names it is a inconsistent data.

Tasks in data preprocessing
 ~Data cleaning: fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies.
   -Fill in missing values (attribute or class value):
        Ignore the tuple: usually done when class label is missing.
        Use the attribute mean (or majority nominal value) to fill in the missing value.
        Use the attribute mean (or majority nominal value) for all samples belonging to the same class.
        Predict the missing value by using a learning algorithm: consider the attribute with the missing value as a dependent (class) variable and run a learning algorithm (usually Bayes or decision tree) to predict the missing value.
   -Identify outliers and smooth out noisy data:
        Binning-Sort the attribute values and partition them into bins (see "Unsupervised discretization" below);
            Then smooth by bin means,  bin median, or  bin boundaries.
        Clustering: group values in clusters and then detect and remove outliers (automatic or manual)
        Regression: smooth by fitting the data into regression functions.
    Correct inconsistent data: use domain knowledge or expert decision.

 ~Data integration: using multiple databases, data cubes, or files.
  A coherent data(Data warehouse)is prepared by collecting data from multiple sources like multiple databases,data cubes or flat    
  files.
 -Schema integration:Integrate metadata from different sources.
 -Detecting and resolving Data value conflicts:As data is collected from multiple sources,attribute values are different for the 
  real world entity.
 -Reduntant data occur due to integration of multiple databases:Attributes maybe represented in different names in different     
  sources of data.It may be derived attribute i.e.yearly income.
 
 ~Data transformation: normalization and aggregation.
  Normalization: Scaling attribute values to fall within a specified range.
  Scaling by using mean and standard deviation  : V'=(V-Mean)/StDev
  Aggregation: moving up in the concept hierarchy on numeric attributes.
  Generalization: moving up in the concept hierarchy on nominal attributes.
  Attribute construction: replacing or adding new attributes inferred by existing attributes.

 ~Data reduction: reducing the volume but producing the same or similar analytical results.
 -Reducing the number of attributes
  Data cube aggregation: applying roll-up, slice or dice operations.
  Removing irrelevant attributes: attribute selection (filtering and wrapper methods), searching the attribute space Attribute-       
  oriented analysis).
  Principle component analysis (numeric attributes only): searching for a lower dimensional space that can best represent the  
  data..
 -Reducing the number of attribute values
  Binning (histograms): reducing the number of attributes by grouping them into intervals (bins).
  Clustering: grouping values in clusters.
  Aggregation or generalization
  -Reducing the number of tuples
   Sampling

 ~Data discretization: part of data reduction, replacing numerical attributes with nominal ones.
  Unsupervised discretization -  class variable is not used.
  Equal-interval (equiwidth) binning: split the whole range of numbers in intervals with equal size.
  Equal-frequency (equidepth) binning: use intervals containing equal number of values.
  Supervised discretization - uses the values of the class variable.
  Using class boundaries. Three steps:
  Sort values.
  Place breakpoints between values belonging to different classes.
  If too many intervals, merge intervals with equal or similar class distributions.
    

**. Partition the given the data into 4 bins using eqi-dept binning method and perform smoothing according to the folowing methods.
Smoothing by bin means
Smoothing by bin median
Smoothing by bin boundries*
**Data : 11, 13, 13,  15, 15, 16, 19, 20, 20, 20, 21, 21, 22, 23, 24, 30, 40, 45, 45, 45, 71, 72, 73, 75**
Partitioning the data set into 4 bins
  Bin 1: 11, 13, 13, 15, 15, 16
  Bin 2: 19, 20, 20, 20, 21, 21
  Bin 3: 22, 23, 24, 30, 40, 45
  Bin 4: 45, 45, 71, 72, 73, 75
Smoothing by bin means:
  Bin 1: 12.17, 12.17, 12.17, 12.17, 12.17, 12.17
  Bin 2: 24.20, 24.20, 24.20, 24.20, 24.20, 24.20
  Bin 3: 30.67, 30.67, 30.67, 30.67, 30.67, 30.67
  Bin 4: 63.50, 63.50, 63.50, 63.50, 63.50, 63.50
Smoothing by bin median:
  Bin 1: 14, 14, 14, 14, 14, 14
  Bin 2: 20, 20, 20, 20, 20, 20
  Bin 3: 27, 27, 27, 27, 27, 27
  Bin 4: 36.50, 36.50, 36.50, 36.50, 36.50, 36.50 
Smoothing by bin boundaries:
  Bin 1: 11, 11, 11, 16, 16, 16
  Bin 2: 19, 19, 19, 19, 21, 21
  Bin 3: 22, 22, 22, 22, 45, 45
  Bin 4: 45, 45, 75, 75, 75, 75

**4. Explain data normalization ?**


