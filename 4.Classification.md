1. A table below shows a sample dataset of whether a customer responds to the survey or not. “Outcome” is a class label. Construct a Decision tree classifier for the dataset. For a new example ( Rural, semidetached, low, No)  what will be the predicted class.

![Table 4.1](/Images/Table_4.1.png)

2. Brifly explain Bagging and Boosting of the Classfiers


3. Define Classification, Issues of Classification and Explain ID3 classfication with example.


4. Explain different methods that can be used to evaluate and compare accuracy of different classfication algorthms.
Using training data to derive a classiﬁer and then estimate the accuracy of the resulting learned model can result in misleading overoptimistic estimates due to overspecialization of the learning algorithm to the data. Instead, it is better to measure the classiﬁer’s accuracy on a test set consisting of class-labeled tuples that were not used to train the model. 
There are four terms that are the “building blocks” used incomputingmanyevaluationmeasures.Understandingthemwillmakeiteasytograsp the meaning of the various measures.

True positives (TP): These refer to the positive tuples that were correctly labeled by the classiﬁer. Let TP be the number of true positives. 

Truenegatives(TN): These are the negative tuples that were correctly labeled by the classiﬁer. Let TN be the number of true negatives. 

False positives (FP): These are the negative tuples that were incorrectly labeled as positive (e.g., tuples of class buys computer = no for which the classiﬁer predicted buys computer=yes). Let FP be the number of false positives. 

False negatives (FN): These are the positive tuples that were mislabeled as negative (e.g., tuples of class buys computer = yes for which the classiﬁer predicted buys computer=no). Let FN be the number of false negatives.
                    
                   Predicted class  
            |       |  yes  |    no  |  Total  
            |  yes  |  TP   |    FN  |   P
Actual Class|       |       |        |
            |  no   |  FP   |    TN  |   N
            |  Total|  P'   |    N'  |  P + N
            
                  CONFUSION MATRIX
A confusion matrix is a table of at least size m by m. An entry, CMi,j in the ﬁrst m rows and m columns indicates the number of tuples of class i that were labeled by the classiﬁer as class j. For a classiﬁer to have good accuracy, ideally most of the tuples would be represented along the diagonal of the confusion matrix, from entry CM1,1 to entry CMm,m, with the rest of the entries being zero or close to zero. That is, ideally, FP and FN are around zero. The table may have additional rows or columns to provide totals. For example, in the confusion matrix of Figure 8.14, P and N are shown. In addition, P0 is the number of tuples that were labeled as positive (TP+FP) and N0 is the number of tuples that werelabeledasnegative(TN+FN).ThetotalnumberoftuplesisTP+TN+FP+TN, or P+N, or P0+N0. Note that although the confusion matrix shown is for a binary classiﬁcation problem, confusion matrices can be easily drawn for multiple classes in a similar manner.    

The accuracy of a classiﬁer on a given test set is the percentage of test set tuples that are correctly classiﬁed by the classiﬁer. That is,
accuracy= (TP+TN)/(P+N)
    
The e errorrate or misclassiﬁcationrate of a classiﬁer, M, which is simply 1−accuracy(M), where accuracy(M) is the accuracy of M. This also can be computed as
error rate= (FP+FN)/(P+N)

If we were to use the training set (instead of a test set) to estimate the error rate of a model, this quantity is known as the resubstitution error. This error estimate is optimistic of the true error rate (and similarly, the corresponding accuracy estimate is optimistic) because the model is not tested on any samples that it has not already seen. 
We now consider the class imbalance problem, where the main class of interest is rare. That is, the data set distribution reﬂects a signiﬁcant majority of the negative class and a minority positive class. For example, in fraud detection applications, the class of interest(orpositiveclass)is“fraud,” whichoccursmuchlessfrequentlythanthenegative “nonfraudulant” class. In medical data, there may be a rare class, such as “cancer.” Suppose that you have trained a classiﬁer to classify medical data tuples, where the class label attribute is “cancer” and the possible class values are “yes” and “no.” An accuracy rate of, say, 97% may make the classiﬁer seem quite accurate, but what if only, say, 3% of the training tuples are actually cancer? Clearly, an accuracy rate of 97% may not be acceptable—the classiﬁer could be correctly labeling only the noncancer tuples, for instance, and misclassifying all the cancer tuples. Instead, we need other measures, which access how well the classiﬁer can recognize the positive tuples (cancer=yes) and how well it can recognize the negative tuples (cancer=no). The sensitivity and speciﬁcity measures can be used, respectively, for this purpose. Sensitivity is also referred to as the true positive (recognition) rate (i.e., the proportion of positive tuples that are correctly identiﬁed), while speciﬁcity is the true negative rate (i.e., the proportion of negative tuples that are correctly identiﬁed). These measures are deﬁned as 
sensitivity= TP/P
speciﬁcity= TN/N
The precision and recall measures are also widely used in classiﬁcation. Precision can be thought of as a measure of exactness (i.e., what percentage of tuples labeled as positive are actually such), whereas recall is a measure of completeness (what percentage ofpositivetuplesarelabeledassuch).Ifrecallseemsfamiliar,that’sbecauseitisthesame as sensitivity (or the true positive rate). These measures can be computed as
precision= TP/(TP+FP)
recall=TP/(TP+FN) = (TP/P)


5. Use any classification techniques and find out the class of (Homeowner = Yes, Status = Employed , Income = Average)

ID,Homeowner,Status,Income, Defaulted
1,Yes,Employed,High,No,
2,No,Business,Average,NO
3,No,Employed,Low,No
4,Yes,Business,High,No
5,No,Unemployed,Average,Yes
6,No,Business,Low,No
7,Yes,Unemployed,High,No
8,No,Employed,Average,Yes
9,No,Business,Low,No
10,No,Employed,Average,Yes

