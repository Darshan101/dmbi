**1. What is hierarchical clustering ? Explain any two techniques for finding distance between the clusters in hierarchical clustering ?**

- Hierarchical methods divides all the data sets into hierarchy or tree structure where their are levels.
- Problem with the hierarchical methods is once the cluster formed you can not undo it.
- This type of clustering is very useful for the data visulization and summarization.


Types of Hierarchical Clustering : Agglomerative or divisive

Agglomerative: 

- An agglomerative hierarchical clustering method uses a bottom-up strategy. 
- It typically starts by letting each object form its own cluster and iteratively merges clusters
  into larger and larger clusters, until all the objects are in a single cluster or certain termination conditions are 	 satisfied
- The single cluster becomes the hierarchy’s root
- For the merging step, it finds the two clusters that are closest to each other
- Two clusters are merged per iteration, an agglomerative method requires at most n iterations.


Divisive :

- Divisive hierarchical clustering method employs a top-down strategy
- It starts by placing all objects in one cluster, which is the hierarchy’s root
- It then divides the root cluster into several smaller subclusters, and recursively partitions those clusters into
smaller ones.
- The partitioning process continues until each cluster at the lowest level is coherent enough—either containing only one object, or the objects within a cluster are sufficiently similar to each other.


![Table 4.1](/Images/Figure_5.1.png)


In both Agglomerative or Divisive hierarchical clustering, a user can specify the desired number of clusters as a termination condition.

![Table 4.1](/Images/Figure_5.2.png)

A tree structure called a dendrogram is commonly used to represent the process of
hierarchical clustering. It shows how objects are grouped together (in an agglomerative
method) or partitioned (in a divisive method) step-by-step.


![Table 4.1](/Images/Figure5.3.png)

Measures for distance between the clusters 




**2. Use any hierarchical clustering algorithm to cluster the following 8 examples into three clusters**

A1 = (2,10)
A2 = (2,5)
A3 = (8,4)
A4 = (5,8)
A5 = (7,5)
A6 = (6,4)
A7 = (1,2)
A8 = (4,9) 

Initial cluster centre A1(2, 10), A4(2,5), A7(1,2)
FIRST ITERATION
	A	B	C
A1	0	3.6	8.06
A2	5	4.24	3.16
A3	8.485	5	7.28
A4	3.605	0	7.211
A5	7.0711	3.606	6.708
A6	7.211	4.123	5.385
A7	8.062	7.211	0
A8	2.236	1.414	7.616

Cluster A (A1)
Cluster B (A4, A3, A5, A6, A8)
Cluster C (A2, A7)

Recalculate cluster centre:  
Cluster A: (2, 10)
Cluster B: (6, 6)
Cluster C: (1.5, 3.5)

SECOND ITERATION
	A	B	C
A1	0	5.65	6.519
A2	5	4.12	1.58
A3	8.48	2.82	6.52
A4	3.6	2.23	5.7
A5	7.07	1.41	5.7
A6	7.2	2	4.52
A7	8.06	6.403	1.58
A8	2.23	3.61	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)

Recalculate cluster centre:  

Cluster A: (3, 9.5)
Cluster B: (6.5, 5.25)
Cluster C: (1.5, 3.5)


THIRD ITERATION
	A	B	C
A1	1.12	6.54	6.52
A2	4.61	4.51	1.58
A3	7.43	1.95	6.52
A4	2.5	3.13	5.7
A5	6.02	0.56	5.7
A6	6.26	1.35	4.53
A7	7.76	6.38	1.58
A8	1.12	4.51	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)
As we can see there is no changes in the cluster this is the final iteration

**3. Clearly explain the DBSCAN algorithm using appropriate algorithms.**

**4. Explain BIRCH algorithm with example**

**5. What is clustering ? Explain k-means clustering alogrithm. Suppose the data for clustering **
   ** { 2, 4, 10, 12, 3, 20, 11, 25 } consider k = 2, cluster the given data using above algorithm.**


**6. Use k-means to cluster the following data into 3 clusters.**

Protin and Fat (20,9), (21,9), (15,7), (22,17), (20,8), (25,12), (26,14), (20,9), (18,9), (20,9)

 


